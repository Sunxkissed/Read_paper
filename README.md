# Paper Reading Notes

## LLM

### Implicit CoT

| Date | Title | Link | Mark |
| ---- | ----- | ---- | ---- |
| 202505 | Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models | [Blog](https://zhuanlan.zhihu.com/p/1888902868641244612) | Survey 1 |
| 202505 | A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond | [Blog](https://blog.csdn.net/yul1024/article/details/147125726) | Survey 2 |
| 202505 | Efficient Reasoning Models: A Survey | [Blog](https://zhuanlan.zhihu.com/p/1895887731894228718) | Survey 3 |
| 202505 | Efficient Inference for Large Reasoning Models: A Survey | - | Survey 4 |
| 202505 | Training Large Language Models to Reason in a Continuous Latent Space | [Github](https://github.com/facebookresearch/coconut) | Coconut (Curriculum learning) |
| 202505 | CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation | - | CODI (Curriculum learning) |
| 202505 | Efficient Reasoning with Hidden Thinking | [Github](https://github.com/shawnricecake/Heima) | Heima (Curriculum learning) (MLLM) |
| 202505 | SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs | [Github](https://github.com/xuyige/SoftCoT) | SoftCoT (Special token) |
| 202505 | Compressed Chain of Thought: Efficient Reasoning through Dense Representations | - | CCoT (Compressed CoT) |
| 202505 | Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning | [Github](https://github.com/lblankl/Token-Assorted) | Token Assorted (Compressed CoT) |




### Overthinking

| Date | Title | Link | Mark |
| ---- | ----- | ---- | ---- |
| 202505 | AdaptThink: Reasoning Models Can Learn When to Think | [Blog](https://zhuanlan.zhihu.com/p/1909543282897313853) [Github](https://github.com/THU-KEG/AdaptThink) | AdaptThink (Think/Nothink) |
| 202505 | Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning | - | CAR (Think/Nothink) |
| 202505 | Reasoning Models Can Be Effective Without Thinking | - | - |
| 202505 | The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks | [Blog](https://www.linkresearcher.com/theses/21271ce1-d44f-4fc3-a0a0-1899d51829e5) | - |
| 202505 | Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens | [Github](https://github.com/chicosirius/think-or-not) | - |
| 202505 | Token-Budget-Aware LLM Reasoning | [Github](https://github.com/GeniusHTX/TALE) | (Adaptive thinking) |
| 202505 | Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens | - | (Adaptive thinking) |
| 20250606 | Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation | [Blog](https://www.themoonlight.io/zh/review/adaptive-inference-time-compute-llms-can-predict-if-they-can-do-better-even-mid-generation) [Blog](https://blog.csdn.net/weixin_46739757/article/details/142925403) | Adaptive sampling |
| 20250606 | ALPHAONE: Reasoning Models Thinking Slow and Fast at Test Time | [Blog](https://mp.weixin.qq.com/s/EIJHZ9yPlDrR3DVlT2pgYg) | Adaptive thinking（引导LRMs先进行充分的慢速思考，然后再快速生成答案，能取得更好的性能） |





### MLLM

| Date | Title | Link | Mark |
| ---- | ----- | ---- | ---- |
| -    | -     | -    | -    |




## AI4Phys

| Date | Title | Link | Mark |
| ---- | ----- | ---- | ---- |
| -    | -     | -    | -    |



## Foundation Models

| Date | Title | Link | Mark |
| ---- | ----- | ---- | ---- |
| -    | -     | -    | -    |



